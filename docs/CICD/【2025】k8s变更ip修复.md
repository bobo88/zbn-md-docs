# Kubernetes 集群 IP 变更修复指南

## 环境信息

**Kubernetes 版本**：

- Client Version: v1.33.2
- Kustomize Version: v5.6.0
- Server Version: v1.33.2

**IP 变更情况**：

- 旧 IP: 192.168.1.11
- 新 IP: 192.168.100.3
- API 服务器端口: 6443

## 修复步骤

### 1. 修改 Hosts 文件

修改所有节点（包括控制平面和工作节点）的 `/etc/hosts` 文件，更新集群 IP 映射。

```bash
# 备份原始 hosts 文件
sudo cp /etc/hosts /etc/hosts.backup

# 编辑 hosts 文件
sudo vi /etc/hosts
```

**修改内容**：

```bash
# 原始配置
192.168.1.11 k8s-master k8s-master.local

# 修改为
192.168.100.3 k8s-master k8s-master.local
```

**验证修改**：

```bash
# 验证 hosts 配置
ping k8s-master

# 在所有节点执行
for node in k8s-node1 k8s-node2; do
  ssh $node "sudo sed -i 's/192.168.1.11/192.168.100.3/g' /etc/hosts"
done
```

### 2. 更新 Kubeconfig 文件

更新 `~/.kube/config` 文件中的 API 服务器地址。

```bash
# 备份原始 kubeconfig
cp ~/.kube/config ~/.kube/config.backup

# 编辑 kubeconfig 文件
vi ~/.kube/config
```

**修改内容**：

```yaml
# 查找并修改 server 字段
clusters:
  - cluster:
      certificate-authority-data: LS0tLS1CRUdJTiB...
      server: https://192.168.100.3:6443 # 修改为新的 IP 地址
    name: kubernetes
```

**批量更新方法**：

```bash
# 方法一：使用 sed 命令
sed -i 's/192.168.1.11:6443/192.168.100.3:6443/g' ~/.kube/config

# 方法二：使用 kubectl 命令
kubectl config set-cluster kubernetes --server=https://192.168.100.3:6443
```

**验证配置**：

```bash
# 测试连接
kubectl cluster-info
kubectl get nodes
```

### 3. 证书相关操作

#### 3.1 备份证书

```bash
# 备份整个证书目录
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki-backup-$(date +%Y%m%d)

# 验证备份
sudo ls -la /etc/kubernetes/pki-backup*/
```

#### 3.2 删除原有证书

```bash
# 删除旧的 API 服务器证书
sudo rm -f /etc/kubernetes/pki/apiserver.*

# 可选：删除其他可能受影响的证书
sudo rm -f /etc/kubernetes/pki/front-proxy-client.*
sudo rm -f /etc/kubernetes/admin.conf
sudo rm -f /etc/kubernetes/kubelet.conf
sudo rm -f /etc/kubernetes/controller-manager.conf
sudo rm -f /etc/kubernetes/scheduler.conf
```

#### 3.3 创建匹配原始设置的配置文件

创建 `kubeadm-config.yaml` 配置文件：

```bash
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.33.2
controlPlaneEndpoint: "192.168.100.3:6443"
apiServer:
  certSANs:
  - "192.168.100.3"      # 新 IP 地址
  - "192.168.1.11"       # 旧 IP 地址（为了兼容性）
  - "127.0.0.1"          # 本地回环
  - "localhost"          # 本地主机名
  - "kubernetes"         # 服务名称
  - "kubernetes.default" # 默认服务域名
  - "kubernetes.default.svc" # 服务完整域名
  - "kubernetes.default.svc.cluster.local" # 集群内域名
  - "10.96.0.1"          # 服务 ClusterIP
  - "$(hostname)"        # 当前主机名
  - "$(hostname -f)"     # 当前 FQDN
imageRepository: "registry.aliyuncs.com/google_containers"
networking:
  podSubnet: "188.188.0.0/16"   # 保持与初始化一致
  serviceSubnet: "10.96.0.0/12"
  dnsDomain: "cluster.local"
certificatesDir: "/etc/kubernetes/pki"
clusterName: "kubernetes"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "192.168.100.3"  # 更新为新 IP
nodeRegistration:
  criSocket: "unix:///run/containerd/containerd.sock"  # 保持与初始化一致
  taints: []
  kubeletExtraArgs:
    node-ip: 192.168.100.3
EOF
```

**验证配置文件**：

```bash
# 检查配置文件
cat kubeadm-config.yaml

# 检查 YAML 格式
yq eval '.' kubeadm-config.yaml
```

#### 3.4 生成新的证书

```bash
# 生成新的 API 服务器证书
sudo kubeadm init phase certs apiserver --config kubeadm-config.yaml

# 输出示例：
# [certs] Generating "apiserver" certificate and key
# [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local localhost] and IPs [10.96.0.1 192.168.100.3 192.168.1.11 127.0.0.1 ::1]
```

#### 3.5 更新代理证书

```bash
# 生成新的前端代理客户端证书
sudo kubeadm init phase certs front-proxy-client --config kubeadm-config.yaml

# 生成新的 etcd 证书（如果是外部 etcd）
sudo kubeadm init phase certs etcd-server --config kubeadm-config.yaml
sudo kubeadm init phase certs etcd-peer --config kubeadm-config.yaml
sudo kubeadm init phase certs etcd-healthcheck-client --config kubeadm-config.yaml
```

#### 3.6 更新 kubeconfig 文件

```bash
# 生成新的 admin kubeconfig
sudo kubeadm init phase kubeconfig admin --config kubeadm-config.yaml

# 生成新的 kubelet kubeconfig
sudo kubeadm init phase kubeconfig kubelet --config kubeadm-config.yaml

# 生成新的 controller-manager kubeconfig
sudo kubeadm init phase kubeconfig controller-manager --config kubeadm-config.yaml

# 生成新的 scheduler kubeconfig
sudo kubeadm init phase kubeconfig scheduler --config kubeadm-config.yaml

# 复制新的 admin 配置到用户目录
sudo cp /etc/kubernetes/admin.conf ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config

# 验证新配置
kubectl config view
```

### 4. 重启 API 服务器

```bash
# 重启 kube-apiserver
sudo systemctl restart kube-apiserver

# 检查 API 服务器状态
sudo systemctl status kube-apiserver

# 查看日志
sudo journalctl -u kube-apiserver -f --no-pager -n 50
```

### 5. 重新安装 Calico 网络插件

#### 5.1 卸载 Calico

```bash
# 删除 Calico 部署
kubectl delete -f https://docs.projectcalico.org/manifests/calico.yaml

# 或者如果使用本地文件
kubectl delete -f calico.yaml

# 等待删除完成
sleep 30

# 强制删除残留资源
kubectl delete daemonset -n kube-system calico-node --ignore-not-found=true
kubectl delete deployment -n kube-system calico-kube-controllers --ignore-not-found=true
kubectl delete customresourcedefinitions.apiextensions.k8s.io \
  bgpconfigurations.crd.projectcalico.org \
  bgppeers.crd.projectcalico.org \
  blockaffinities.crd.projectcalico.org \
  clusterinformations.crd.projectcalico.org \
  felixconfigurations.crd.projectcalico.org \
  globalnetworkpolicies.crd.projectcalico.org \
  globalnetworksets.crd.projectcalico.org \
  hostendpoints.crd.projectcalico.org \
  ipamblocks.crd.projectcalico.org \
  ipamconfigs.crd.projectcalico.org \
  ipamhandles.crd.projectcalico.org \
  ippools.crd.projectcalico.org \
  ipreservations.crd.projectcalico.org \
  kubecontrollersconfigurations.crd.projectcalico.org \
  networkpolicies.crd.projectcalico.org \
  networksets.crd.projectcalico.org
```

#### 5.2 清理 Calico 配置信息

```bash
# 清理网络接口
sudo ip link delete cali0 2>/dev/null || true
sudo ip link delete tunl0 2>/dev/null || true

# 清理 iptables 规则
sudo iptables-save | grep -v calico | sudo iptables-restore
sudo ip6tables-save | grep -v calico | sudo ip6tables-restore

# 清理网络命名空间
sudo ip netns list | grep calico | xargs -I {} sudo ip netns delete {} 2>/dev/null || true

# 清理路由表
sudo ip route show | grep calico | while read route; do
  sudo ip route del $route
done

# 清理配置文件
sudo rm -rf /etc/cni/net.d/10-calico.conflist
sudo rm -rf /etc/cni/net.d/calico-kubeconfig
sudo rm -rf /opt/cni/bin/calico
sudo rm -rf /opt/cni/bin/calico-ipam
sudo rm -rf /var/lib/calico
```

#### 5.3 安装 Calico 网络插件

```bash
# 下载 Calico 配置文件
curl -O https://docs.projectcalico.org/manifests/calico.yaml

# 或者使用本地修改过的配置文件
# 确保配置文件中的 IP 地址范围与集群配置一致

# 查看配置文件中的关键配置
grep -A5 -B5 "192.168.100.3" calico.yaml || true
grep -A5 -B5 "188.188.0.0" calico.yaml || true

# 应用 Calico 配置
kubectl apply -f calico.yaml

# 或者使用 Tigera Operator（推荐）
kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
```

### 6. 验证集群状态

```bash
# 等待 Calico Pod 启动
sleep 60

# 检查所有 Pod 状态
kubectl get pods --all-namespaces -o wide

# 检查节点状态
kubectl get nodes -o wide

# 检查网络连通性
kubectl run test-pod --image=busybox --rm -it --restart=Never -- sh
# 在容器内执行：ping 10.96.0.1

# 检查服务
kubectl get svc -A

# 检查端点
kubectl get endpoints -A
```

### 7. 更新工作节点配置

如果集群中有工作节点，需要在每个工作节点上执行以下操作：

#### 7.1 更新工作节点 hosts 文件

```bash
# 在每个工作节点上执行
sudo sed -i 's/192.168.1.11/192.168.100.3/g' /etc/hosts
```

#### 7.2 更新工作节点 kubelet 配置

```bash
# 在工作节点上编辑 kubelet 配置
sudo vi /var/lib/kubelet/kubeadm-flags.env

# 修改 --node-ip 参数（如果有）
# 原始：--node-ip=192.168.1.xx
# 修改：--node-ip=192.168.100.xx
```

#### 7.3 重启工作节点 kubelet

```bash
# 重启 kubelet 服务
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 检查状态
sudo systemctl status kubelet
```

#### 7.4 在主节点上重新签发证书

```bash
# 在主节点上为工作节点重新生成证书
sudo kubeadm init phase certs renew all

# 或者删除并重新加入节点
# 1. 在工作节点上执行
sudo kubeadm reset
# 2. 在主节点上获取加入命令
kubeadm token create --print-join-command
# 3. 在工作节点上执行加入命令
```

### 8. 更新其他组件配置

#### 8.1 更新 CoreDNS 配置

```bash
# 检查 CoreDNS 配置
kubectl get configmap -n kube-system coredns -o yaml | grep -A10 ".:53"

# 如果 CoreDNS 使用旧 IP，可能需要重建
kubectl delete pod -n kube-system -l k8s-app=kube-dns
```

#### 8.2 更新 Ingress 控制器

```bash
# 检查 Ingress 控制器配置
kubectl get svc -n ingress-nginx

# 如果使用 LoadBalancer，可能需要更新外部 IP
kubectl edit svc -n ingress-nginx ingress-nginx-controller
```

#### 8.3 更新存储配置

```bash
# 检查 StorageClass
kubectl get storageclass

# 如果使用 NFS 或其他基于 IP 的存储，更新配置
kubectl edit pv <persistent-volume-name>
```

### 9. 故障排除

#### 9.1 常见问题及解决方案

**问题 1：API 服务器无法启动**

```bash
# 查看错误日志
sudo journalctl -u kube-apiserver -n 100 --no-pager

# 常见错误：证书验证失败
# 解决方案：重新生成证书
sudo rm -rf /etc/kubernetes/pki/apiserver.*
sudo kubeadm init phase certs apiserver --config kubeadm-config.yaml
```

**问题 2：节点 NotReady**

```bash
# 检查 kubelet 日志
sudo journalctl -u kubelet -n 100 --no-pager

# 常见错误：连接 API 服务器失败
# 解决方案：检查网络和 hosts 配置
ping 192.168.100.3
telnet 192.168.100.3 6443
```

**问题 3：Pod 网络不通**

```bash
# 检查 Calico 组件
kubectl get pods -n kube-system | grep calico

# 检查网络策略
kubectl get networkpolicy -A

# 检查 IP 池
kubectl get ippools.crd.projectcalico.org -o yaml
```

#### 9.2 诊断命令

```bash
# 集群健康检查
kubectl get cs

# 组件状态
kubectl get componentstatus

# 事件查看
kubectl get events --sort-by='.lastTimestamp' -A

# 网络诊断
kubectl run net-test --image=nicolaka/netshoot --rm -it --restart=Never
```

### 10. 备份与恢复计划

#### 10.1 创建备份

```bash
# 创建完整的集群备份
BACKUP_DIR="/backup/k8s-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

# 备份 etcd
if kubectl get pods -n kube-system | grep -q etcd; then
  kubectl exec -n kube-system etcd-$(hostname) -- \
    etcdctl snapshot save /tmp/etcd-snapshot.db
  kubectl cp -n kube-system etcd-$(hostname):/tmp/etcd-snapshot.db $BACKUP_DIR/etcd-snapshot.db
fi

# 备份关键配置
cp -r /etc/kubernetes $BACKUP_DIR/kubernetes-config
cp -r /var/lib/kubelet $BACKUP_DIR/kubelet-data
kubectl get all -A -o yaml > $BACKUP_DIR/all-resources.yaml

# 压缩备份
tar -czf $BACKUP_DIR.tar.gz $BACKUP_DIR
```

#### 10.2 恢复步骤

```bash
# 如果出现问题，从备份恢复
tar -xzf k8s-backup.tar.gz
cd k8s-backup

# 恢复证书
sudo cp -r kubernetes-config/pki /etc/kubernetes/

# 恢复配置
sudo cp kubernetes-config/*.conf /etc/kubernetes/

# 恢复 etcd（如果需要）
kubectl exec -n kube-system etcd-$(hostname) -- \
  etcdctl snapshot restore /tmp/etcd-snapshot.db \
  --data-dir /var/lib/etcd-from-backup
```

## 注意事项

1. **操作顺序**：严格按照上述步骤顺序执行
2. **备份先行**：所有操作前务必备份关键数据
3. **逐步验证**：每个步骤后验证操作效果
4. **时间窗口**：选择业务低峰期进行操作
5. **回滚计划**：准备好快速回滚方案

## 完成验证清单

- [ ] Hosts 文件已更新
- [ ] Kubeconfig 文件已更新
- [ ] 证书已重新生成
- [ ] API 服务器正常启动
- [ ] 所有节点状态正常
- [ ] 网络插件正常运行
- [ ] Pod 网络连通性正常
- [ ] 服务访问正常
- [ ] 存储访问正常
- [ ] 监控系统正常

完成以上步骤后，Kubernetes 集群应该已经成功迁移到新的 IP 地址，所有功能恢复正常。
